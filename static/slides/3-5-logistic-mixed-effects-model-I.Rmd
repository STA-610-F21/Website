---
title: "STA 610L: Module 3.5"
subtitle: "Logistic mixed effects model (Part I)"
author: "Dr. Olanrewaju Michael Akande"
date: " "
output:
  xaringan::moon_reader:
    css: "slides.css"
    logo: img/class_logo.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
library(knitr)
knitr::opts_chunk$set(fig.height = 2.65, dpi = 300,fig.align='center',fig.show='hold',size='footnotesize', small.mar=TRUE) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(rvest)
#library(tufte)
#library(sjPlot)
library(lme4)
#library(sjstats)
#library(merTools)
#library(lattice)
#library(ggplot2)
#library(brms)
#library(dutchmasters)
#library(tidyverse)
#library(rethinking)
library(arm)
```



## Generalized linear mixed effects model (GLMM)

As we continue to generalize the concepts we have covered, let's think about the incorporation of random effects into the standard representation of generalized linear models.

The basic idea is that we assume there is natural heterogeneity across groups in a subset of the regression coefficients.

These coefficients are assumed to vary across groups according to some distribution.

Conditional on the random effects, we then assume the responses for a single subject are independent observations from a distribution in the exponential family.




---
## GLMM

Note: when we look at longitudinal data, we will group by $i$, otherwise, we will group by $j$.

.hlight[Generalized linear mixed effects models (GLMMs)] for longitudinal data usually take the form 
$$g(E[Y_{ij} \mid {\bf X}_{ij}, {\bf Z}_{ij}, \boldsymbol{\beta}, {\bf b}_i])={\bf X}_{ij}'\boldsymbol{\beta}+{\bf Z}_{ij}'{\bf b}_i,$$
so that we assume the conditional distribution of each $Y_{ij}$, conditional on ${\bf b}_i$ (and everything else), belongs to the exponential family with the conditional mean given above, where $g(\cdot)$ is a known link function.

From here on, I will often write $E[Y_{ij} \mid {\bf X}_{ij}, {\bf Z}_{ij}, \boldsymbol{\beta}, {\bf b}_i]$ as $E[Y_{ij} \mid {\bf b}_i]$ for brevity.

We assume the ${\bf b}_i$ are independent across subjects with ${\bf b}_i \overset{iid}\sim N({\bf 0}, {\bf D})$.

We also assume that given ${\bf b}_i$, the responses $Y_{i1}, \ldots, Y_{in}$ are mutually independent.




---
## Example: multilevel linear regression with random intercepts

$$Y_{ij}={\bf X}_{ij}'\boldsymbol{\beta}+b_i + \varepsilon_{ij},$$

where
$$b_i\overset{iid}\sim N(0,\sigma_b^2) \perp \varepsilon_{ij} \overset{iid}\sim N(0,\sigma_e^2)$$

and
$$E(Y_{ij} \mid b_i)={\bf X}_{ij}'\boldsymbol{\beta}+b_i$$



---
## Example: multilevel logistic model with random intercepts

$$\text{logit}(E(Y_{ij} \mid b_i))={\bf X}_{ij}'\boldsymbol{\beta}+b_i,$$

where
$$b_i\sim N(0,\sigma^2)$$

<br>

Question:  what happened to $\varepsilon_{ij}$?




---
## Example:  multilevel Poisson model

$$\log(E(Y_{ij} \mid {\bf b}_i))={\bf X}_{ij}'\boldsymbol{\beta}+{\bf Z}_{ij}'{\bf b}_i.$$

We could write
$${\bf X}_{ij}={\bf Z}_{ij}=[1,t_{ij}],$$

so we have random slopes and intercepts and then assume

$${\bf b}_i \sim N({\bf 0}, {\bf D}).$$




---
## Interpretation of GLMM estimates

In the model
$$\text{logit}(E[Y_{ij} \mid b_i])={\bf X}_{ij}'\boldsymbol{\beta}+b_i,$$

with $b_i \sim N(0,\sigma^2)$, each element of $\boldsymbol{\beta}$ measures the change in the log odds of a 'positive' response per unit change in the respective covariate, in a specific group that has an underlying propensity to respond positively given by $b_i$.

<br>

That is, we need to hold the group membership constant when interpreting $\beta_k$, just as we would hold the values of $\bf{x}_{i,(-k)}$ constant when interpreting $\beta_k$



---
## Caution

Note also that with a non-linear link function, a non-linear contrast of the averages is not equal to the average of non-linear contrasts, so that
the parameters do not in general have population-average interpretations (as they would in a linear mixed effects model, which has identity link).

So while in the lmm
$$g(E(Y_{ij} \mid {\bf X}_{ij}, {\bf b}_i))={\bf X}_{ij}'\boldsymbol{\beta}+{\bf Z}_{ij}'{\bf b}_i$$

implies that $E(Y_{ij} \mid {\bf X}_{ij})={\bf X}_{ij}'\boldsymbol{\beta}$, when $g(\cdot)$ is non-linear (say the logit), then the same relationship does not hold and
$$g(E(Y_{ij} \mid {\bf X}_{ij}))\neq {\bf X}_{ij}'\boldsymbol{\beta},$$

for all $\boldsymbol{\beta}$ when averaged over the distribution of the random effects.



---
## Intraclass correlation

Let's focus on binary response for the rest of this module. That is, each $Y_{ij} \in \{0,1\}$.

Now consider an unobserved continuous variable $W_{ij}$.

$W_{ij}$ is related to $Y_{ij}$ in the following manner: $Y_{ij}=1$ if $W_{ij}<c$, and $Y_{ij}=0$  otherwise.

<br>

The location of $c$ and the distribution of $W$ govern the probability that $Y=1$.



---
## Intraclass correlation

Useful way of thinking about the model (but not an essential assumption) is:
$$W_{ij}= {\bf X}_{ij}'\boldsymbol{\beta}+b_i+\varepsilon_{ij}$$

  - $\varepsilon_{ij} \sim N(0,1)$:  probit regression
  
  - $\varepsilon_{ij} \sim$ standard logistic (mean 0, variance $\frac{\pi^2}{3}$):  logistic regression

<br>

We can use this framework to calculate ICC's:
  
  - $ICC=\frac{\sigma^2}{\sigma^2+1} ~~~ \text{for probit}$
  
  - $ICC=\frac{\sigma^2}{\sigma^2+\frac{\pi^2}{3}} ~~~ \text{for logistic}$





---
## Estimation using ML

The joint probability density function is given by $$f({\bf Y}_i \mid {\bf X}_i, {\bf b}_i)f({\bf b}_i).$$  However, recall that the ${\bf b}_i$ are unobserved.

How then do we handle the ${\bf b}_i$ in the maximization? 

<br>

Typically, we base frequentist inferences on the marginal (integrated) likelihood function, given by
$$\prod_{i=1}^N \int f({\bf Y}_i \mid {\bf X}_i, {\bf b}_i)f({\bf b}_i) d{\bf b}_i.$$

Estimation using maximum likelihood then involves a two-step procedure.



---
## ML estimation steps

.hlight[Step 1]: Obtain ML estimates of $\boldsymbol{\beta}$ and ${\bf D}$ based on the marginal likelihood of the data.

  While this may sound simple, numerical or Monte Carlo integration techniques are typically required, and the techniques used may have substantial impacts on resulting inferences.

.hlight[Step 2]: Given estimates of $\boldsymbol{\beta}$ and ${\bf D}$, predict the random effects as
$$\widehat{{\bf b}}_i=E({\bf b}_i \mid {\bf Y}_i, \widehat{\boldsymbol{\beta}}, \widehat{{\bf D}}).$$

Again, simple analytic solutions are rarely available.


<br>

Even when the burden of integration is not that great, the optimization problem may be difficult to solve.



---
## Random effects logistic regression

Inverse logit functions for random intercepts logistic model with a single predictor.

```{r fig.height=4,echo=F,warning=F}
curve(invlogit((0.3*x)),xlim=c(0,5),ylim=c(0,1),col="blue4",ylab="probabilities")
curve(invlogit((1+0.3*x)),xlim=c(0,5),ylim=c(0,1),col="red4",add=T)
curve(invlogit((1.5+0.3*x)),xlim=c(0,5),ylim=c(0,1),col="green4",add=T)
curve(invlogit((3+0.3*x)),xlim=c(0,5),ylim=c(0,1),col="orange4",add=T)
```



---
## Random effects logistic regression

Inverse logit functions for random slopes logistic model with a single predictor.

```{r fig.height=4,echo=F,warning=F}
curve(invlogit((0.3*x)),xlim=c(0,5),ylim=c(0,1),col="blue4",ylab="probabilities")
curve(invlogit((0.5*x)),xlim=c(0,5),ylim=c(0,1),col="red4",add=T)
curve(invlogit((0.7*x)),xlim=c(0,5),ylim=c(0,1),col="green4",add=T)
curve(invlogit((x)),xlim=c(0,5),ylim=c(0,1),col="orange4",add=T)
```



---
## Random effects logistic regression

Inverse logit functions for random intercepts and random slopes logistic model with a single predictor.

```{r fig.height=4,echo=F,warning=F}
curve(invlogit((0.3*x)),xlim=c(0,5),ylim=c(0,1),col="blue4",ylab="probabilities")
curve(invlogit((1+0.5*x)),xlim=c(0,5),ylim=c(0,1),col="red4",add=T)
curve(invlogit((1.5-0.7*x)),xlim=c(0,5),ylim=c(0,1),col="green4",add=T)
curve(invlogit((3-x)),xlim=c(0,5),ylim=c(0,1),col="orange4",add=T)
```



---
## 1988 elections analysis

To illustrate how to fit and interpret the results of random effect logistic models, we will use a sample data on election polls.

National opinion polls are conducted by a variety of organizations (e.g., media, polling organizations, campaigns) leading up to elections.

While many of the best opinion polls are conducted at a national level, it can also be often interesting to estimate voting opinions and preferences at the state or even local level.

Well-designed polls are generally based on national random samples with corrections for nonresponse based on a variety of demographic factors (e.g., sex, ethnicity, race, age, education).

The data is from CBS News surveys conducted during the week before the 1988 election.

Respondents were asked about their preferences for either the Republican candidate (Bush Sr.) or the Democratic candidate (Dukakis). 



---
## 1988 elections analysis

The dataset includes 2193 observations from one of eight surveys (the most recent CBS News survey right before the election) in the original full data.

.small[
Variable    | Description
:------------- | :------------
org | cbsnyt = CBS/NYT
.hlight[bush] | .hlight[1 = preference for Bush Sr., 0 = otherwise]
state | 1-51: 50 states including DC (number 9)
edu | education: 1=No HS, 2=HS, 3=Some College, 4=College Grad
age | 1=18-29, 2=30-44, 3=45-64, 4=65+
female | 1=female, 0=male
black | 1=black, 0=otherwise
region | 1=NE, 2=S, 3=N, 4=W, 5=DC
v_prev | average Republican vote share in the three previous elections (adjusted for home-state and home-region effects in the previous elections)
]

Given that the data has a natural multilevel structure (through `state` and `region`), it makes sense to explore hierarchical models for this data.



---
## 1988 elections analysis

Both voting turnout and preferences often depend on a complex combination of demographic factors.

In our example dataset, we have demographic factors such as biological sex, race, age, education, which we may all want to look at by state, resulting in $2 \times 2 \times 4 \times 4 \times 51 = 3264$ potential categories of respondents.

We may even want to control for `region`, adding to the number of categories.

Clearly, without a very large survey (most political survey poll around 1000 people), we will need to make assumptions in order to even obtain estimates in each category.

We usually cannot include all interactions; we should therefore select those to explore (through EDA and background knowledge).

The data is in the file `polls_subset.txt` on Sakai.



---
## 1988 elections analysis

```{r}
###### Load the data
polls_subset <- read.table("data/polls_subset.txt",header=TRUE)
str(polls_subset)
head(polls_subset)
```


---
## 1988 elections analysis

```{r}
summary(polls_subset)
```



---
## 1988 elections analysis

```{r}
polls_subset$v_prev <- polls_subset$v_prev*100 #rescale 
polls_subset$region_label <- factor(polls_subset$region,levels=1:5,
                                    labels=c("NE","S","N","W","DC"))
#we consider DC as a separate region due to its distinctive voting patterns
polls_subset$edu_label <- factor(polls_subset$edu,levels=1:4,
                                 labels=c("No HS","HS","Some College","College Grad"))
polls_subset$age_label <- factor(polls_subset$age,levels=1:4,
                                 labels=c("18-29","30-44","45-64","65+"))
#the data includes states but without the names, which we will need, 
#so let's grab that from R datasets
data(state) 
#"state" is an R data file (type ?state from the R command window for info)
state.abb #does not include DC, so we will create ours
#In the polls data, DC is the 9th "state" in alphabetical order
state_abbr <- c (state.abb[1:8], "DC", state.abb[9:50])
polls_subset$state_label <- factor(polls_subset$state,levels=1:51,labels=state_abbr)
rm(list = ls(pattern = "state")) #remove unnecessary values in the environment
```



---
## 1988 elections analysis

```{r}
###### View properties of the data  
head(polls_subset)
dim(polls_subset)
```



---
## 1988 elections analysis

```{r}
###### View properties of the data  
str(polls_subset)
```




---
## 1988 elections analysis

I will not do any meaningful EDA here.

I expect you to be able to do this yourself.

Let's just take a look at the amount of data we have for "bush" and the age:edu interaction.

```{r}
###### Exploratory data analysis
table(polls_subset$bush) #well split by the two values
table(polls_subset$edu,polls_subset$age)
```


---
## 1988 elections analysis

As a start, we will consider a simple model with fixed effects of race and sex and a random effect for state (50 states + the District of Columbia).
$$
\begin{split}
\textrm{bush}_{ij} | \boldsymbol{x}_{ij} & \sim \textrm{Bernoulli}(\pi_{ij}); \ \ \ i = 1, \ldots, n; \ \ \ j = 1, \ldots, J=51; \\
\textrm{log}\left(\dfrac{\pi_{ij}}{1-\pi_{ij}}\right) & = \beta_{0} + b_{0j} + \beta_1 \textrm{female}_{ij} + \beta_2 \textrm{black}_{ij}; \\
b_{0j} & \sim N(0, \sigma^2).
\end{split}
$$

In `R`, we have
```{r results = 'hide'}
#library(lme4)
model1 <- glmer(bush ~ black+female+(1|state_label),
                family=binomial(link="logit"),
                data=polls_subset)
summary(model1)
```



---
## 1988 elections analysis
```{r echo=F}
summary(model1)
```


---
## 1988 elections analysis

Looks like we dropped some NAs.
```{r}
c(sum(complete.cases(polls_subset)),sum(!complete.cases(polls_subset)))
```

Not ideal; we'll learn about methods for dealing with missing data soon.

Interpretation of results:
  + For a fixed state (or across all states), a non-black male respondent has odds of $e^{0.45}=1.57$ of supporting Bush.

  + For a fixed state and sex, a black respondent as $e^{-1.74}=0.18$ times (an 82% decrease)  the odds of supporting Bush as a non-black respondent; you are much less likely to support Bush if your race is black compared to being non-black.
  
  + For a given state and race, a female respondent has $e^{-0.10}=0.91$ (a 9% decrease)  times the odds of supporting Bush as a male respondent. However, this effect is not actually statistically significant!



---
## 1988 elections analysis

The state-level standard deviation is estimated at 0.41, so that the states do vary some, but not so much.

I expect that you will be able to interpret the corresponding confidence intervals.
```{r echo=F}
confint(model1)
```


---
## 1988 elections analysis

We can definitely fit a more sophisticated model that includes other relevant survey factors, such as

  + region
  
  + prior vote history (note that this is a state-level predictor),
  
  + age, education, and the interaction between them.

Given the structure of the data, it makes sense to include region as a second (nested) grouping variable.

We are yet to discuss that, so I will return to this later.




---
## 1988 elections analysis

For now, let's just fit two models, one with the main effects for age and education, and the second with the interaction between them.

.large[
```{r, cache=T, echo=F}
model2 <- glmer(bush ~ black + female + edu_label + age_label +(1|state_label),
                family=binomial(link="logit"),data=polls_subset)
print(summary(model2),correlation = FALSE)
```
]

Can you interpret the results?


---
## 1988 elections analysis

```{r, cache=T, results = 'hide'}
model3 <- glmer(bush ~ black + female + edu_label*age_label + (1|state_label),
                family=binomial(link="logit"),data=polls_subset)
```

Why do we have a rank deficient model? Also, it looks like we have a convergence issue.

These issues can happen. We have so many parameters to estimate from the interaction terms `edu_label*age_label` (16 actually), and it looks like that's causing a problem. 



---
## Note on estimation

ML estimation is carried out typically using adaptive Gaussian quadrature.

To improve accuracy over many package defaults (Laplace approximation), increase the number of quadrature points to be greater than one.

Note that some software packages (including the .hlight[glmer] function in the .hlight[lme4] package) require Laplace approximation with Gaussian quadrature if the number of random effects is more than 1 for the sake of computational efficiency.

It is possible to tweak the optimizer in the .hlight[glmer] function in particular. Read more about the [BOBYQA](https://en.wikipedia.org/wiki/BOBYQA) optimizer at your leisure.




---
## 1988 elections analysis

Let’s fit a more sophisticated model that includes other relevant survey factors, such as 

+ region (note here that states are nested within regions)
  
+ prior vote history (note that this is a state-level predictor),
  
+ age, education, and the interaction between them.

We can start with
```{r results = 'hide',cache=T}
model2 <- glmer(bush ~ black + female + v_prev + edu_label + age_label +
                (1|state_label) + (1|region_label),
                family=binomial(link="logit"),data=polls_subset)
```



---
## 1988 elections analysis

From the statement of the problem, we also would like to include the interaction between `edu_label` and `age_label`.

However, recall we had the convergence issues because there are so many parameters to estimate from the interaction terms (16 actually).

Could be that we have too many $\textrm{bush}_i = 1$ or $0$ values for certain combinations? You should check!

Let's also treat those as varying effects instead. That is,
```{r results = 'hide',cache=T}
model3 <- glmer(bush ~ black + female + v_prev + 
                  (1|state_label) + (1|region_label) + 
                  (1|edu_label:age_label),
                family=binomial(link="logit"),data=polls_subset)
```


This seems to run fine; we are able to borrow information which helps.



---
## 1988 elections analysis

.small[
```{r echo=F}
summary(model3)
```
]



---
## 1988 elections analysis

Remember that in the first model, the state-level standard deviation was estimated as 0.41. Looks like we are now able to separate that (for the most part) into state and region effects.

Interpretation of results:

+ For a fixed state, education and age bracket, a non-black male respondent with zero prior average Republican vote share, has odds of $e^{-3.51}=0.03$ of supporting Bush (no one really has 0 value for `v_prev`).
  
+ For a fixed state, sex, education level, age bracket and zero prior average Republican vote share, a black respondent has $e^{-1.75}=0.17$ times (an 83% decrease) the odds of supporting Bush as a non-black respondent, which is about the same as before.
  
+ For each percentage point increase in prior average Republican vote share, residents of a given state, race, sex, education level  age bracket have $e^{0.07}=1.07$ times the odds of supporting Bush.





---

class: center, middle

# What's next?

### Move on to the readings for the next module!




