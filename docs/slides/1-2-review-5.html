<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>STA 610L: Review</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr. Olanrewaju Michael Akande" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# STA 610L: Review
## Important distributions
### Dr. Olanrewaju Michael Akande

---








## Gaussian (normal) distribution

A random variable `\(X\)` has a Gaussian (normal) distribution with mean `\(\mu\)` and variance `\(\sigma^2\)`, written `\(X \sim N(\mu,\sigma^2)\)`, if `\(X\)` has density

`\begin{equation*}
f(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left\{-\frac{1}{2 \sigma^2}(x-\mu)^2\right\}.
\end{equation*}`

When `\(\mu=0\)` and `\(\sigma=1\)`, then we say `\(X \sim N(0,1)\)` has a *standard normal distribution*.  

If `\(X \sim N(\mu,\sigma^2)\)`, then `\(\frac{x-\mu}{\sigma} \sim N(0,1)\)`.  

For the standard normal distribution, 95% of the probability mass falls between -1.96 and 1.96 (roughly 2 standard deviations of the mean).  

Much of hypothesis testing is based on this fact.

The normal distribution is symmetric about its mean.



---
## Standard normal distribution

&lt;img src="img/stdnormal.jpg" width="650px" height="500px" style="display: block; margin: auto;" /&gt;




---
## Multivariate Gaussian distribution

`\(\text{Suppose }X=(X_1,\ldots,X_n)'. X\)` has an `\(n\)` dimensional multivariate Gaussian (normal) distribution with `\(\text{mean } \mu \text{ and covariance } \Sigma, ~~ X \sim N_n(\mu,\Sigma)\)`,  if `\(X\)` has density
`\begin{equation*}
f(x)=\frac{1}{(2\pi)^{\frac{n}{2}} \mid \Sigma \mid^{\frac{1}{2}}} \exp\left\{-\frac{1}{2} (x-\mu)' \Sigma^{-1} (x-\mu)\right\}.
\end{equation*}`


&lt;img src="img/mvnormal.jpg" width="550px" height="400px" style="display: block; margin: auto;" /&gt;



---
## Helpful facts

  - A linear transformation of a multivariate normal distribution yields another multivariate normal distribution. 
  
  Suppose `\(X \sim N_n(\mu,\Sigma)\)`.  For `\(A_{r \times n}\)`, a matrix of constants and `\(b_{r \times 1}\)`, a vector of constants, then `\(Y=AX+b\)` has the multivariate normal distribution given by `\(Y \sim N_r(A \mu+b,A \Sigma A')\)`.  
  
  - A linear combination of independent multivariate normal distributions is a multivariate normal distribution.  
  
  Suppose `\(X_1, \ldots, X_k\)` are independent with `\(X_i \sim N_n(\mu_i,\Sigma_i)\)`, `\(i=1,\ldots,k\)`.  
  
  Suppose `\(a_1, \ldots, a_k\)` are scalars and define `$$Y=a_1 X_1 + \ldots + a_k X_k.$$` 
  
  Then `\(Y \sim N(\mu^*,\Sigma^*)\)`, where `\(\mu^*=\sum_{i=1}^k a_i \mu_i\)` and `\(\Sigma^*=\sum_{i=1}^k a_i^2 \Sigma_i\)`.




---
## Helpful facts

- Partition `\(\boldsymbol{X} = (X_1,\ldots,X_n)'\)` as
`\begin{eqnarray*}
\boldsymbol{X} =
\begin{pmatrix}\boldsymbol{X}_1\\
\boldsymbol{X}_2
\end{pmatrix} &amp; \sim &amp; N_n\left[\left(\begin{array}{c}
\boldsymbol{\mu}_1\\
\boldsymbol{\mu}_2
\end{array}\right),\left(\begin{array}{cc}
\Sigma_{11} &amp; \Sigma_{12} \\
\Sigma_{21} &amp; \Sigma_{22}
\end{array}\right)\right],\\
\end{eqnarray*}`

  where
  + `\(\boldsymbol{X}_1\)` and `\(\boldsymbol{\mu}_1\)` are `\(r \times 1\)`,
  + `\(\boldsymbol{X}_2\)` and `\(\boldsymbol{\mu}_2\)` are `\((n-r) \times 1\)`,
  + `\(\Sigma_{11}\)` is `\(r \times r\)`, and
  + `\(\Sigma_{22}\)` is `\((n-r) \times (n-r)\)`, with `\(\Sigma_{22} &gt; 0\)`.
  
--

- Then,
`$$\boldsymbol{X}_1 | \boldsymbol{X}_2 = \boldsymbol{x}_2 \sim N_r\left(\boldsymbol{\mu}_1 + \Sigma_{12}\Sigma_{22}^{-1}  (\boldsymbol{x}_2-\boldsymbol{\mu}_2), \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}\right).$$`

--

- Marginal distributions are also normal, that is,
`$$\boldsymbol{X}_1  \sim N_r\left(\boldsymbol{\mu}_1, \Sigma_{11}\right); \ \ \ \boldsymbol{X}_2  \sim N_{n-r}\left(\boldsymbol{\mu}_2, \Sigma_{22}\right).$$`



---
## Why do we care?

We often make assumptions in hierarchical models that certain quantities follow normal distributions -- these could be univariate or multivariate responses, random effects, or prior parameters. 

Sometimes we might assume certain quantities follow conditional normal distributions as well. 

Stay tuned for more details!


---
## Chi-squared distribution

A random variable `\(X\)` has a central chi-squared distribution with `\(n\)` degrees of freedom, written `\(X \sim \chi^2(n), \text{ if the density of } X\)` is given by
`\begin{equation*}
f(x)=\left(\frac{1}{\Gamma(\frac{n}{2})}\right) \left(\frac{1}{2}\right)^{\frac{n}{2}} x^{\frac{n}{2}-1} \exp\left\{-\frac{x}{2}\right\},
\end{equation*}`
where `\(\Gamma(a)\)` is the complete gamma function, given by `\(\Gamma(a)=\int_0^\infty x^{a-1} e^{-x} dx\)`.  

The chi-squared distribution is asymmetric and restricted to positive numbers.  

Its degrees of freedom determine the mean and variance of the distribution.


---
## Chi-squared distribution

&lt;img src="img/chisquared.jpg" width="650px" height="500px" style="display: block; margin: auto;" /&gt;



---
## Chi-squared distribution
The chi-squared distribution is related to the normal distribution.  

If the random variable `\(Z \sim N(0,1)\)`, then `\(Z^2 \sim \chi^2(1)\)`.  

In addition, if `\(Z_1, Z_2, \ldots, Z_n\)` are independent, identically distributed `\(N(0,1)\)` random variables, then `\(W=\sum_{i=1}^n Z_i^2\)` has a chi-squared distribution with `\(n\)` degrees of freedom; that is, `\(W \sim \chi^2(n)\)`.  

The mean of a `\(\chi^2(n)\)` distribution is `\(n\)`, and its variance is `\(2n\)`.


---
## Why do we care?

The chi-squared distribution comes up a lot in testing using frequentist hierarchical and multilevel models, and mixtures of chi-squared distributions are important in testing variance components in frequentist multilevel models. 


---
## Student's t distribution

This distribution has a [great story](https://priceonomics.com/the-guinness-brewer-who-revolutionized-statistics/)!  




Suppose `\(X \sim N(0,1), Y \sim \chi^2(n) \text{ with } X \text{ and } Y\)` independent.  The random variable
`\begin{equation*}
T=\frac{X}{\sqrt{\frac{Y}{n}}}
\end{equation*}`
has a `\(t\)` distribution with `\(n\)` degrees of freedom.  

We write this as `\(T \sim t(n)\)`. Like the standard normal, the t distribution is symmetric about 0.



---
## Student's t distribution

&lt;img src="img/t.jpg" width="650px" height="500px" style="display: block; margin: auto;" /&gt;




---
## Student's t distribution


The degrees of freedom `\(n\)` determines the amount of variability in the `\(t\)` distribution.  

As the number of degrees of freedom increases, the variability of the `\(t\)` distribution decreases.  

In fact, as the number of degrees of freedom gets large, the `\(t\)` distribution approximates the standard normal distribution.  

With smaller degrees of freedom, the t distribution resembles a normal distribution with fatter tails.


A `\(t(1)\)` distribution, which has 1 degree of freedom, is not well-behaved and is called a *Cauchy distribution*.


---
##  Why do we care?

Ahh, we see the t-distribution a lot -- it's a standard distribution in frequentist testing when we use 1 df tests in linear models and ANOVA.


---
## Fisher's F distribution

In case you're getting tired and feeling demoralized by this review of distributions, you might want to take a break and read about a [big mistake in Fisher's understanding](https://priceonomics.com/why-the-father-of-modern-statistics-didnt-believe/).  Even a brilliant scientist can make huge mistakes, so take heart!


---
## Fisher's F distribution

Suppose `\(X_1 \sim \chi^2(n_1) \text{ and } X_2 \sim \chi^2(n_2)\)`, and `\(X_1 \text{ and } X_2\)` are independent.  The random variable 
`\begin{equation*}
F=\frac{\left(\frac{X_1}{n_1}\right)}{\left(\frac{X_2}{n_2}\right)}
\end{equation*}`
has a central `\(F\)` distribution with `\((n_1,n_2)\)` degrees of freedom.  We write this `\(F \sim F(n_1,n_2)\)`.




---
## Fisher's F distribution

![](img/f.jpg){width=45%}

We call `\(n_1\)` the numerator degrees of freedom and `\(n_2\)` the denominator degrees of freedom. If `\(T \sim t(\nu)\)`, then `\(T^2 \sim F_{1,\nu}\)`. The `\(F\)` distribution is asymmetric and restricted to positive numbers.


---
## Why do we care?

The F distribution is the workhorse of testing hypotheses about the mean in linear models and ANOVA. 




---

class: center, middle

# What's next? 

### Move on to the readings for the next module!




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
